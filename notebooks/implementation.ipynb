{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafio LATAM\n",
    "\n",
    "Por Juan PINEDA-JARAMILLO\n",
    "https://github.com/jdpinedaj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problema\n",
    "\n",
    "El problema consiste en tomar el trabajo previo realizado por el Data Scientist y exponerlo para que sea explotado por un sistema\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desarrollo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Importando las librerías y leyendo datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime as dt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as imbPipeline\n",
    "from joblib import dump, load\n",
    "\n",
    "# sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# settings to display all columns\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7663/1540584960.py:2: DtypeWarning: Columns (1,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('../data/dataset_SCL.csv')\n"
     ]
    }
   ],
   "source": [
    "# Se lee el dataset original\n",
    "df = pd.read_csv('../data/dataset_SCL.csv')\n",
    "\n",
    "\n",
    "# Se generan las columnas adicionales, utilizando las siguientes funciones\n",
    "def temporada_alta(fecha):\n",
    "    \"\"\"\n",
    "    Función que calcula la temporada de alta de una fecha\n",
    "    \"\"\"\n",
    "    fecha_año = int(fecha.split('-')[0])\n",
    "    fecha = dt.strptime(fecha, '%Y-%m-%d %H:%M:%S')\n",
    "    range1_min = dt.strptime('15-Dec', '%d-%b').replace(year=fecha_año)\n",
    "    range1_max = dt.strptime('31-Dec', '%d-%b').replace(year=fecha_año)\n",
    "    range2_min = dt.strptime('1-Jan', '%d-%b').replace(year=fecha_año)\n",
    "    range2_max = dt.strptime('3-Mar', '%d-%b').replace(year=fecha_año)\n",
    "    range3_min = dt.strptime('15-Jul', '%d-%b').replace(year=fecha_año)\n",
    "    range3_max = dt.strptime('31-Jul', '%d-%b').replace(year=fecha_año)\n",
    "    range4_min = dt.strptime('11-Sep', '%d-%b').replace(year=fecha_año)\n",
    "    range4_max = dt.strptime('30-Sep', '%d-%b').replace(year=fecha_año)\n",
    "\n",
    "    if ((fecha >= range1_min and fecha <= range1_max)\n",
    "            or (fecha >= range2_min and fecha <= range2_max)\n",
    "            or (fecha >= range3_min and fecha <= range3_max)\n",
    "            or (fecha >= range4_min and fecha <= range4_max)):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def dif_min(data):\n",
    "    \"\"\"\n",
    "    Función que calcula la diferencia de minutos entre una fecha y otra\n",
    "    \"\"\"\n",
    "    fecha_o = dt.strptime(data['Fecha-O'], '%Y-%m-%d %H:%M:%S')\n",
    "    fecha_i = dt.strptime(data['Fecha-I'], '%Y-%m-%d %H:%M:%S')\n",
    "    dif_min = ((fecha_o - fecha_i).total_seconds()) / 60\n",
    "    return dif_min\n",
    "\n",
    "\n",
    "def get_periodo_dia(fecha):\n",
    "    \"\"\"\n",
    "    Función que calcula el periodo de una fecha\n",
    "    \"\"\"\n",
    "    fecha_time = dt.strptime(fecha, '%Y-%m-%d %H:%M:%S').time()\n",
    "    mañana_min = dt.strptime(\"05:00\", '%H:%M').time()\n",
    "    mañana_max = dt.strptime(\"11:59\", '%H:%M').time()\n",
    "    tarde_min = dt.strptime(\"12:00\", '%H:%M').time()\n",
    "    tarde_max = dt.strptime(\"18:59\", '%H:%M').time()\n",
    "    noche_min1 = dt.strptime(\"19:00\", '%H:%M').time()\n",
    "    noche_max1 = dt.strptime(\"23:59\", '%H:%M').time()\n",
    "    noche_min2 = dt.strptime(\"00:00\", '%H:%M').time()\n",
    "    noche_max2 = dt.strptime(\"4:59\", '%H:%M').time()\n",
    "\n",
    "    if (fecha_time > mañana_min and fecha_time < mañana_max):\n",
    "        return 'mañana'\n",
    "    elif (fecha_time > tarde_min and fecha_time < tarde_max):\n",
    "        return 'tarde'\n",
    "    elif ((fecha_time > noche_min1 and fecha_time < noche_max1)\n",
    "          or (fecha_time > noche_min2 and fecha_time < noche_max2)):\n",
    "        return 'noche'\n",
    "\n",
    "\n",
    "# Crerando las columnas adicionales, aplicando las funciones creadas anteriormente\n",
    "df['temporada_alta'] = df['Fecha-I'].apply(temporada_alta)\n",
    "df['dif_min'] = df.apply(dif_min, axis=1)\n",
    "df['atraso_15'] = np.where(df['dif_min'] > 15, 1, 0)\n",
    "df['periodo_dia'] = df['Fecha-I'].apply(get_periodo_dia)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformación de las variables realizadas por el data scientist\n",
    "\n",
    "# Shuffle dataframe\n",
    "\n",
    "data = shuffle(df[['OPERA', 'MES', 'TIPOVUELO', 'atraso_15']],\n",
    "               random_state=111)\n",
    "\n",
    "# Separar los datos de entrenamiento y prueba\n",
    "features = data.drop(['atraso_15'], axis=1)\n",
    "label = data['atraso_15']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Escoger el modelo que a tu criterio tenga un mejor performance, argumentando la decisión.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El data scientist entrenó un modelo de regresión logística y un XGBoost sencillos, verificando que su performance era muy malo.\n",
    "Para mejorarlo, el data scientist optó por aplicar dos medidas:\n",
    "\n",
    "- Encontrar los mejores hiperparámetros del modelo XGBoost mediante la aplicación de la metodología Grid Search.\n",
    "- Balancear la clase minoritaria mediante la aplicación de la técnica de oversampling.\n",
    "\n",
    "Esta elecci[on del data scientist es la mejor según el criterio de evaluación que ha elegido, por lo que se toma esta opción como inicial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como el data scientist no imprimió los mejores parámetros que le daba el modelo,\n",
    "# se realiza aquí nuevamente el grid search utilizando su misma técnica.\n",
    "\n",
    "# Train test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(features,\n",
    "                                                    label,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=42)\n",
    "\n",
    "# Creación del Pipeline\n",
    "## División de variables en numéricas y categóricas\n",
    "categorical_features = ['OPERA', 'TIPOVUELO', 'MES']\n",
    "\n",
    "## Definición de Transformadores\n",
    "categorical_transformer = Pipeline(steps=[('encoder', OneHotEncoder())])\n",
    "\n",
    "## Definición del preprocesador\n",
    "preprocessor = ColumnTransformer(transformers=[('categorical',\n",
    "                                                categorical_transformer,\n",
    "                                                categorical_features)])\n",
    "\n",
    "## Definición del Pipeline\n",
    "pipeline = Pipeline(\n",
    "    steps=[('preprocessor', preprocessor),\n",
    "           ('classifier',\n",
    "            xgb.XGBClassifier(random_state=1, learning_rate=0.01))])\n",
    "\n",
    "# # Convirtiendo los features en un dataframe, conservando los nombres de las columnas como prefijos\n",
    "# features = pd.DataFrame(features.toarray(),\n",
    "#                         columns=enc.get_feature_names(\n",
    "#                             ['OPERA', 'TIPOVUELO', 'MES']))\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "modelxgb = pipeline.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;categorical&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;encoder&#x27;,\n",
       "                                                                   OneHotEncoder())]),\n",
       "                                                  [&#x27;OPERA&#x27;, &#x27;TIPOVUELO&#x27;,\n",
       "                                                   &#x27;MES&#x27;])])),\n",
       "                (&#x27;classifier&#x27;,\n",
       "                 XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "                               colsample_bylevel=1, colsample_bynode=1,\n",
       "                               colsample_bytree=1, early_stopping_rounds=None,\n",
       "                               enable_categorical=False, eval_met...\n",
       "                               gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "                               importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "                               learning_rate=0.01, max_bin=256,\n",
       "                               max_cat_to_onehot=4, max_delta_step=0,\n",
       "                               max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "                               missing=nan, monotone_constraints=&#x27;()&#x27;,\n",
       "                               n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "                               predictor=&#x27;auto&#x27;, random_state=1, reg_alpha=0,\n",
       "                               reg_lambda=1, ...))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;categorical&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;encoder&#x27;,\n",
       "                                                                   OneHotEncoder())]),\n",
       "                                                  [&#x27;OPERA&#x27;, &#x27;TIPOVUELO&#x27;,\n",
       "                                                   &#x27;MES&#x27;])])),\n",
       "                (&#x27;classifier&#x27;,\n",
       "                 XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "                               colsample_bylevel=1, colsample_bynode=1,\n",
       "                               colsample_bytree=1, early_stopping_rounds=None,\n",
       "                               enable_categorical=False, eval_met...\n",
       "                               gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "                               importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "                               learning_rate=0.01, max_bin=256,\n",
       "                               max_cat_to_onehot=4, max_delta_step=0,\n",
       "                               max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "                               missing=nan, monotone_constraints=&#x27;()&#x27;,\n",
       "                               n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "                               predictor=&#x27;auto&#x27;, random_state=1, reg_alpha=0,\n",
       "                               reg_lambda=1, ...))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;categorical&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;encoder&#x27;, OneHotEncoder())]),\n",
       "                                 [&#x27;OPERA&#x27;, &#x27;TIPOVUELO&#x27;, &#x27;MES&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">categorical</label><div class=\"sk-toggleable__content\"><pre>[&#x27;OPERA&#x27;, &#x27;TIPOVUELO&#x27;, &#x27;MES&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder()</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "              importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "              learning_rate=0.01, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=1,\n",
       "              reg_alpha=0, reg_lambda=1, ...)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('categorical',\n",
       "                                                  Pipeline(steps=[('encoder',\n",
       "                                                                   OneHotEncoder())]),\n",
       "                                                  ['OPERA', 'TIPOVUELO',\n",
       "                                                   'MES'])])),\n",
       "                ('classifier',\n",
       "                 XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "                               colsample_bylevel=1, colsample_bynode=1,\n",
       "                               colsample_bytree=1, early_stopping_rounds=None,\n",
       "                               enable_categorical=False, eval_met...\n",
       "                               gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "                               importance_type=None, interaction_constraints='',\n",
       "                               learning_rate=0.01, max_bin=256,\n",
       "                               max_cat_to_onehot=4, max_delta_step=0,\n",
       "                               max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "                               missing=nan, monotone_constraints='()',\n",
       "                               n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "                               predictor='auto', random_state=1, reg_alpha=0,\n",
       "                               reg_lambda=1, ...))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelxgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 18 candidates, totalling 36 fits\n",
      "[[18320    83]\n",
      " [ 3960   145]]\n",
      "{'classifier__learning_rate': 0.05, 'classifier__n_estimators': 150, 'classifier__subsample': 0.9}\n"
     ]
    }
   ],
   "source": [
    "# Definición de parámetros para implementar el grid search\n",
    "\n",
    "parameters = {\n",
    "    'classifier__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'classifier__n_estimators': [50, 100, 150],\n",
    "    'classifier__subsample': [0.5, 0.9]\n",
    "}\n",
    "\n",
    "modelxgb_GridCV = GridSearchCV(modelxgb,\n",
    "                               param_grid=parameters,\n",
    "                               cv=2,\n",
    "                               n_jobs=-1,\n",
    "                               verbose=1).fit(x_train, y_train)\n",
    "\n",
    "# Predicción del modelo\n",
    "y_predxgb_grid = modelxgb_GridCV.predict(x_test)\n",
    "\n",
    "# Confusion matrix\n",
    "cfm = confusion_matrix(y_test.tolist(), y_predxgb_grid.tolist())\n",
    "print(cfm)\n",
    "\n",
    "# Imprimiendo los mejores parámetros\n",
    "print(modelxgb_GridCV.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanceo de datos usando misma metodología que el data scientist\n",
    "\n",
    "data_no_retraso = data[data['atraso_15'] == 0]\n",
    "data_atraso = data[data['atraso_15'] == 1]\n",
    "\n",
    "# upsampling\n",
    "data_atraso_upsampled = resample(\n",
    "    data_atraso,\n",
    "    replace=True,  # sample with replacement\n",
    "    n_samples=30000,  # to match majority class\n",
    "    random_state=42)  # reproducible results\n",
    "\n",
    "data_upsampled = pd.concat([data_no_retraso, data_atraso_upsampled])\n",
    "\n",
    "features_upsampled = data_upsampled.drop(['atraso_15'], axis=1)\n",
    "label_upsampled = data_upsampled['atraso_15']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17315  1034]\n",
      " [ 8198  1699]]\n"
     ]
    }
   ],
   "source": [
    "x_upsampled_train, x_upsampled_test, y_upsampled_train, y_upsampled_test = train_test_split(\n",
    "    features_upsampled, label_upsampled, test_size=0.33, random_state=42)\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "pipeline.fit(x_upsampled_train, y_upsampled_train)\n",
    "y_upsampled_predxgb = modelxgb.predict(x_upsampled_test)\n",
    "cfm_upsampled = confusion_matrix(y_upsampled_test.tolist(),\n",
    "                                 y_upsampled_predxgb.tolist())\n",
    "print(cfm_upsampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.94      0.79     18349\n",
      "           1       0.62      0.17      0.27      9897\n",
      "\n",
      "    accuracy                           0.67     28246\n",
      "   macro avg       0.65      0.56      0.53     28246\n",
      "weighted avg       0.66      0.67      0.61     28246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_upsampled_test, y_upsampled_predxgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implementar mejoras sobre el modelo escogiendo la o las técnicas que prefieras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las métricas obtenidas por el data scientist no son las mejores, considerando que lo que se busca predecir la probabilidad de que un vuelo esté atrasado, y este modelo tiene un recall del 0.19, lo cual es bastante malo. Así pues, es mejor tener un modelo que incrementé el recall para la clase que se busca predecir (en este caso la probabilidad de atraso de un vuelo, representado por el número: 1).\n",
    "\n",
    "En un reto anterior de Data Scientist, utilizando datos adicionales de aeropuertos obtenidos en https://www.kaggle.com/datasets/jinbonnie/airport-information?resource=download, y creando variables adicionales como el conteo de número de vuelos atrasados por destino, número de vuelos atrasados por operador, número de vuelos atrasados por periodo del día, número de vuelos atrasados por periodo del día-mes-día, distancia entre el aeropuerto de Santiago de Chile y múltiples destinos (usando la fórmula de Haversine) logré entrenar un simple modelo de regresión logística con un recall de casi el 75%, muy superior al logrado por el data scientist.\n",
    "\n",
    "Para más información, por favor mirar el notebook que creé en el reto anterior que se encuentra en mi repositorio: https://github.com/jdpinedaj/desafio_latam/blob/master/notebooks/solution.ipynb\n",
    "\n",
    "Con el objetivo de no repetir todo el proceso que realicé ahí, que incluye incorporar nuevos datos y utilizar modelos distintos, tan solamente cambiaré algunos parámetros implementados allí para mejorar el modelo de XgBoost construido por el data scientist, siguiendo la filosofía de este desafío.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OPERA</th>\n",
       "      <th>MES</th>\n",
       "      <th>TIPOVUELO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60899</th>\n",
       "      <td>Grupo LATAM</td>\n",
       "      <td>11</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18403</th>\n",
       "      <td>Sky Airline</td>\n",
       "      <td>4</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53808</th>\n",
       "      <td>Grupo LATAM</td>\n",
       "      <td>10</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36508</th>\n",
       "      <td>Grupo LATAM</td>\n",
       "      <td>7</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53445</th>\n",
       "      <td>Grupo LATAM</td>\n",
       "      <td>10</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52990</th>\n",
       "      <td>Grupo LATAM</td>\n",
       "      <td>10</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27973</th>\n",
       "      <td>Copa Air</td>\n",
       "      <td>6</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60298</th>\n",
       "      <td>Grupo LATAM</td>\n",
       "      <td>11</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10840</th>\n",
       "      <td>Grupo LATAM</td>\n",
       "      <td>2</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48086</th>\n",
       "      <td>Grupo LATAM</td>\n",
       "      <td>9</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45698 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             OPERA  MES TIPOVUELO\n",
       "60899  Grupo LATAM   11         I\n",
       "18403  Sky Airline    4         N\n",
       "53808  Grupo LATAM   10         N\n",
       "36508  Grupo LATAM    7         N\n",
       "53445  Grupo LATAM   10         N\n",
       "...            ...  ...       ...\n",
       "52990  Grupo LATAM   10         N\n",
       "27973     Copa Air    6         I\n",
       "60298  Grupo LATAM   11         I\n",
       "10840  Grupo LATAM    2         I\n",
       "48086  Grupo LATAM    9         N\n",
       "\n",
       "[45698 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.52      0.65     18403\n",
      "           1       0.25      0.70      0.36      4105\n",
      "\n",
      "    accuracy                           0.55     22508\n",
      "   macro avg       0.57      0.61      0.51     22508\n",
      "weighted avg       0.77      0.55      0.60     22508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Modificar el pipeline para incluir SMOTE\n",
    "\n",
    "sm = SMOTE()\n",
    "\n",
    "# Creación del Pipeline\n",
    "categorical_features = ['OPERA', 'TIPOVUELO', 'MES']\n",
    "categorical_transformer = Pipeline(steps=[('encoder', OneHotEncoder())])\n",
    "preprocessor = ColumnTransformer(transformers=[('categorical',\n",
    "                                                categorical_transformer,\n",
    "                                                categorical_features)])\n",
    "\n",
    "## Definición del Pipeline, usando make_pipeline para imblearn como un paso\n",
    "pipeline = imbPipeline(steps=[('preprocessor', preprocessor), (\n",
    "    'smote',\n",
    "    sm), ('classifier',\n",
    "          xgb.XGBClassifier(random_state=1, learning_rate=0.01))])\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "modelxgb_smote = pipeline.fit(x_train, y_train)\n",
    "\n",
    "# Predicción del modelo\n",
    "y_predxgb_smote = modelxgb_smote.predict(x_test)\n",
    "\n",
    "print(classification_report(y_test, y_predxgb_smote))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Pipeline.get_params of Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('categorical',\n",
       "                                                  Pipeline(steps=[('encoder',\n",
       "                                                                   OneHotEncoder())]),\n",
       "                                                  ['OPERA', 'TIPOVUELO',\n",
       "                                                   'MES'])])),\n",
       "                ('smote', SMOTE()),\n",
       "                ('classifier',\n",
       "                 XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "                               colsample_bylevel=1, colsample_bynode=1,\n",
       "                               colsample_bytree=1, early_stopping_rounds=None,\n",
       "                               enable_categori...\n",
       "                               gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "                               importance_type=None, interaction_constraints='',\n",
       "                               learning_rate=0.01, max_bin=256,\n",
       "                               max_cat_to_onehot=4, max_delta_step=0,\n",
       "                               max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "                               missing=nan, monotone_constraints='()',\n",
       "                               n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "                               predictor='auto', random_state=1, reg_alpha=0,\n",
       "                               reg_lambda=1, ...))])>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelxgb_smote.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[[10367  8036]\n",
      " [ 1359  2746]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.56      0.69     18403\n",
      "           1       0.25      0.67      0.37      4105\n",
      "\n",
      "    accuracy                           0.58     22508\n",
      "   macro avg       0.57      0.62      0.53     22508\n",
      "weighted avg       0.77      0.58      0.63     22508\n",
      "\n",
      "{'classifier__subsample': 0.5, 'classifier__n_estimators': 100, 'classifier__min_child_weight': 6, 'classifier__max_depth': 8, 'classifier__learning_rate': 0.01, 'classifier__gamma': 0.2, 'classifier__colsample_bytree': 0.6}\n"
     ]
    }
   ],
   "source": [
    "# Aplicación de la metodología Random Search para búsqueda de parámetros,\n",
    "# puesto que esta metodología es más optima que el grid search.\n",
    "\n",
    "# Definición de parámetros para implementar el random search\n",
    "\n",
    "parameters = {\n",
    "    'classifier__learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    'classifier__n_estimators': [50, 75, 100, 125, 150],\n",
    "    'classifier__subsample': [0.5, 0.6, 0.7, 0.8],\n",
    "    'classifier__max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'classifier__min_child_weight': [1, 2, 3, 4, 5, 6],\n",
    "    'classifier__gamma': [0.0, 0.1, 0.2, 0.3, 0.4],\n",
    "    'classifier__colsample_bytree': [0.5, 0.6, 0.7]\n",
    "}\n",
    "\n",
    "modelxgb_smote_RandomCV = RandomizedSearchCV(\n",
    "    modelxgb_smote,\n",
    "    param_distributions=parameters,\n",
    "    cv=5,  # Se incrementa el número de folds para que sea más robusto\n",
    "    n_jobs=-1,\n",
    "    n_iter=20,\n",
    "    scoring=\n",
    "    'recall',  # Se usa recall para que sea más robusto para la predicción de atrasos de vuelos\n",
    "    verbose=1).fit(x_train, y_train)\n",
    "\n",
    "# Predicción del modelo\n",
    "y_predxgb_smote_rdm = modelxgb_smote_RandomCV.predict(x_test)\n",
    "\n",
    "# Confusion matrix\n",
    "cfm_rdm = confusion_matrix(y_test.tolist(), y_predxgb_smote_rdm.tolist())\n",
    "print(cfm_rdm)\n",
    "\n",
    "print(classification_report(y_test, y_predxgb_smote_rdm))\n",
    "\n",
    "# Imprimiendo los mejores parámetros\n",
    "print(modelxgb_smote_RandomCV.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí se puede ver que las mejoras realizadas al data scientist mejoran las métricas (principalmente recall, que sería lo más importante según lo comentado previamente).\n",
    "Sin embargo, y tal cual lo comenté anteriormente, en mi desafío anterior (https://github.com/jdpinedaj/desafio_latam/blob/master/notebooks/solution.ipynb) se consigue un mejor modelo mediante la utilización de datos adicionales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/modelxgb_smote_RandomCV_final.pkl']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aquí se vuelve a entrenar el nuevo y mejorado modelo xgboost al total de los datos,\n",
    "# usando los mejores hiperparámetros encontrados mediante la aplicación de la metodología Random Search\n",
    "#  y el oversampling con SMOTE\n",
    "\n",
    "pipeline_final = imbPipeline(\n",
    "    steps=[('preprocessor', preprocessor), ('smote', sm),\n",
    "           ('classifier',\n",
    "            xgb.XGBClassifier(random_state=1,\n",
    "                              learning_rate=modelxgb_smote_RandomCV.\n",
    "                              best_params_['classifier__learning_rate'],\n",
    "                              n_estimators=modelxgb_smote_RandomCV.\n",
    "                              best_params_['classifier__n_estimators'],\n",
    "                              subsample=modelxgb_smote_RandomCV.\n",
    "                              best_params_['classifier__subsample'],\n",
    "                              max_depth=modelxgb_smote_RandomCV.\n",
    "                              best_params_['classifier__max_depth'],\n",
    "                              min_child_weight=modelxgb_smote_RandomCV.\n",
    "                              best_params_['classifier__min_child_weight'],\n",
    "                              gamma=modelxgb_smote_RandomCV.\n",
    "                              best_params_['classifier__gamma'],\n",
    "                              colsample_bytree=modelxgb_smote_RandomCV.\n",
    "                              best_params_['classifier__colsample_bytree']))])\n",
    "\n",
    "modelxgb_smote_RandomCV_final = pipeline_final.fit(features, label)\n",
    "\n",
    "# Se guarda el modelo\n",
    "\n",
    "dump(modelxgb_smote_RandomCV_final,\n",
    "     \"../models/modelxgb_smote_RandomCV_final.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('desafio_latam_ML-4FPGH1Vc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63f3f5459acd55f67f2ae5863efbfe8cb1abd7da45d197e3a81ee5e878e4029f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
